spark-shell --master local --packages org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.0


import org.apache.spark.sql._
import org.apache.spark.sql.types._ 

val articleSchema = new StructType()
 .add("author", "string")
 .add("title", "string")
 .add("description", "string")
 .add("url", "string")
 .add("urlToImage", "string")
 .add("publishedAt", "string")
 .add("content", "string")
 .add("source.id", "string")
 .add("source.name", "string")

// Read streaming data
val articles = spark.readStream.format("json")
 .schema(articleSchema)
 .option("path", "hdfs:///final_assignment/")
 .load()

// Add a timestamp column for window-based aggregations
val articlesWithTimestamp = articles
  .withColumn("event_time", current_timestamp())

// Add watermark and perform window-based aggregation for articles per author
val articlesPerAuthor = articlesWithTimestamp
  .withWatermark("event_time", "10 seconds")
  .groupBy(
    col("author"),
    window(col("event_time"), "10 seconds")
  )
  .count()
  .withColumnRenamed("count", "articles_count")

// Count of articles per source
val articlesPerSource = articlesWithTimestamp
  .withWatermark("event_time", "10 seconds") // Watermark
  .groupBy(
    col("`source.name`")
    window(col("event_time"), "10 seconds")
  )
  .count()
  .withColumnRenamed("count", "articles_count")

// Write aggregated results to file sink
val authorStream = articlesPerAuthor.writeStream
  .format("json")
  .option("path", "file:///home/aakashguru6898/final_assignment/output_author") // Correct path
  .option("checkpointLocation", "file:///home/aakashguru6898/final_assignment/chkpt_author")
  .outputMode("append") // Append mode
  .start()

val sourceStream = articlesPerSource.writeStream
  .format("json")
  .option("path", "file:///home/aakashguru6898/final_assignment/output_source")
  .option("checkpointLocation", "file:///home/aakashguru6898/final_assignment/chkpt_source")
  .outputMode("append")
  .start()

// Await termination
authorStream.awaitTermination()
sourceStream.awaitTermination()

